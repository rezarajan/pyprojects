# AI Agent Configuration

# LLM model to use (requires OPENAI_API_KEY env var)
model: gpt-4o-mini

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
temperature: 0.2

# Maximum tokens in LLM response
max_output_tokens: 1600

# Future options:
# - planner_depth: number of planning iterations
# - write_mode: dry-run vs apply
# - test_generation: enabled/disabled
